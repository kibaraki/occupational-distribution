{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18561,"status":"ok","timestamp":1685706550531,"user":{"displayName":"Katsumi Ibaraki","userId":"03674733650401449323"},"user_tz":240},"id":"PzXTnowdvbqa","outputId":"059fb6fd-9e78-4f59-dda5-b485638ff119"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Collecting aiohttp (from openai)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Installing collected packages: tokenizers, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, transformers, aiohttp, openai\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 openai-0.27.7 tokenizers-0.13.3 transformers-4.29.2 yarl-1.9.2\n","/bin/bash: conda: command not found\n","/bin/bash: conda: command not found\n"]}],"source":["!pip install openai transformers\n","!conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia\n","!conda install ipywidgets -c conda-forge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErdonnFSwCzy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9880,"status":"ok","timestamp":1685706560937,"user":{"displayName":"Katsumi Ibaraki","userId":"03674733650401449323"},"user_tz":240},"id":"BY5HjqMjwNc0","outputId":"eea6529e-e08d-4e38-a18b-6b7d058bc44d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ben5VHxYw7O0"},"outputs":[],"source":["# import sentencepiece\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmQxK4Z2wG0P"},"outputs":[],"source":["# rinna GPT-2\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n","\n","# model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZJyBvWBAhS2"},"outputs":[],"source":["# # gpt-2 medium\n","\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n","\n","# model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8Y17OWzAlL6"},"outputs":[],"source":["# # mGPT\n","\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n","\n","# model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVd5YxQcAsXF"},"outputs":[],"source":["# # NeoX\n","\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n","\n","# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Kmgj-PXBkjw"},"outputs":[],"source":["# # NeoX-Japanese\n","\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b\")\n","\n","# model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVpAyDCjB09F"},"outputs":[],"source":["# # BERT m\n","\n","# from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LskfCAkB5d_"},"outputs":[],"source":["# # BERT Japanese\n","\n","# from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n","\n","# model = AutoModelForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21779,"status":"ok","timestamp":1685706584250,"user":{"displayName":"Katsumi Ibaraki","userId":"03674733650401449323"},"user_tz":240},"id":"BhCUqpx2RoDL","outputId":"345204f0-d10c-427b-9d77-64e2fae38d7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhGJJU5Xx8Am"},"outputs":[],"source":["men_pmpt = pd.read_csv('/content/drive/MyDrive/bias in LMs/constrained decoding/prompt and occ csv/men_bert_prompt_new.csv')\n","wom_pmpt = pd.read_csv('/content/drive/MyDrive/bias in LMs/constrained decoding/prompt and occ csv/wom_bert_prompt_new.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCs_rmgfzUbX"},"outputs":[],"source":["# occupations = pd.read_csv('occupations_2020.csv')  # to 1920\n","# occ = occupations['職業']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1gtkgJv590v"},"outputs":[],"source":["# remove duplicates and drop certain occ in occupations df\n","\n","# occupations = pd.read_csv('小分類_1920_T9.csv')  # to 1920\n","# #occ = occupations['職業']\n","# print(len(occupations.index))\n","# occupations = occupations[occupations['職業'].str.contains('分類不能') == False]\n","# occupations = occupations[occupations['職業'].str.contains('その他') == False]\n","# occupations.drop_duplicates(subset=['職業'])\n","# print(len(occupations.index))\n","\n","# occupations.to_csv('occupations_1920.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXeuY-H66jT_"},"outputs":[],"source":["# occupations.to_csv('occupations_2020.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqLpo9Js12zM"},"outputs":[],"source":["# def get_score(prompt, job):\n","#   input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","#   input_ids = input.input_ids\n","\n","#   force = tokenizer([job], add_special_tokens=False).input_ids\n","#  # print(force)\n","#   outputs = model.generate(\n","#       input_ids,\n","#       force_words_ids=force,\n","#       num_beams=5,\n","#       max_new_tokens = 5,\n","#       return_dict_in_generate=True,\n","#       output_scores=True\n","#       )\n","\n","#   transition_scores = model.compute_transition_scores(\n","#       outputs.sequences, outputs.scores, normalize_logits=True\n","#   )\n","\n","#   input_length = input_ids.shape[1]\n","#   generated_tokens = outputs.sequences[:, input_length:]\n","#   print(transition_scores)\n","\n","#   true_score = -9999\n","\n","#   for tok, score in zip(generated_tokens[0], transition_scores[0]):\n","#       # | token | token string | logits | probability\n","#       # print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score:.3f} | \")\n","#       if tokenizer.decode(tok) == job:\n","#         true_score = score\n","\n","#  # if force in \n","\n","#   # input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n","#   # generated_tokens = outputs.sequences[:, input_length:]\n","#   # logit = transition_scores[0][0].cpu().item()  # logit of first token\n","\n","#   # force_index = generated_tokens.index(force[0].item())\n","#   # logit = transition_scores[force_index]\n","\n","#   return true_score.cpu().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXQyXObWKz_V"},"outputs":[],"source":["# def get_score(prompt, job):\n","#   input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","#   input_ids = input.input_ids\n","\n","#   force = tokenizer([job], add_special_tokens=False).input_ids\n","#   #print(force[0][1]) #.cpu())\n","#   outputs = model.generate(\n","#       input_ids,\n","#       force_words_ids=force,\n","#       num_beams=5,\n","#       max_new_tokens = 10,\n","#       # num_return_sequences = 1,\n","#       return_dict_in_generate=True,\n","#       output_scores=True\n","#       )\n","\n","#   transition_scores = model.compute_transition_scores(\n","#       outputs.sequences, outputs.scores, normalize_logits=True\n","#   )\n","\n","#   input_length = input_ids.shape[1]\n","#   # print(input_length)\n","#   generated_tokens = outputs.sequences[:, input_length:]\n","#   # print(transition_scores)\n","\n","#   # # print(generated_tokens[0]) #.cpu())\n","#   #for tok, score in zip(generated_tokens[0], transition_scores[0]):\n","#       #print(tokenizer.decode(tok))\n","#       # if tokenizer.decode(tok) == job:\n","#       #   true_score = score\n","\n","# #   t = torch.Tensor([1, 2, 3])\n","# # print ((t == 2).nonzero(as_tuple=True)[0])\n","\n","#   t = generated_tokens[0]\n","#   # print(t)\n","#   # print(force[0][1])\n","\n","#   try:\n","#     index = ((t == force[0][1]).nonzero(as_tuple=True)[0])[0].item() \n","#   except:\n","#     return -100\n","\n","  \n","#   # print(tokenizer.decode(generated_tokens[0][index].item()))\n","\n","#   # index =  force[0][1] # force.cpu()\n","#   # # index = generated_tokens[0].item()\n","#   # # print(index)\n","\n","#   # if true_score == -9999:\n","#   #   return true_score\n","\n","#   # return true_score.cpu().item()\n","\n","#   return transition_scores[0][index].cpu().item() #.[0]][index].cpu().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjnxJI-cAW_H"},"outputs":[],"source":["def get_score(prompt, job, model, tokenizer):\n","  input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","  input_ids = input.input_ids\n","\n","  force = tokenizer([job], add_special_tokens=False).input_ids\n","  outputs = model.generate(\n","      input_ids,\n","      force_words_ids=force,\n","      num_beams=5,\n","      max_new_tokens = 10,\n","      return_dict_in_generate=True,\n","      output_scores=True\n","      )\n","\n","  transition_scores = model.compute_transition_scores(\n","      outputs.sequences, outputs.scores, normalize_logits=True\n","  )\n","\n","  input_length = input_ids.shape[1]\n","  generated_tokens = outputs.sequences[:, input_length:]\n","\n","  t = generated_tokens[0]\n","\n","  try:\n","    index = ((t == force[0][1]).nonzero(as_tuple=True)[0])[0].item() \n","  except:\n","    return -100\n","\n","  return transition_scores[0][index].cpu().item() #.[0]][index].cpu().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QT49gus82o_K"},"outputs":[],"source":["# test get_score\n","\n","# a = get_score('太郎の仕事は', '医者')\n","# # b = get_score('太郎の仕事は', '助産師')\n","# # c = get_score('太郎の仕事は', '保育')\n","\n","# print(a)\n","# # print(b)\n","# # print(c)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOX8rI5z51pJ"},"outputs":[],"source":["# tensor(-11.7443, device='cuda:0')\n","# tensor(-11.9802, device='cuda:0')\n","# tensor(-6.8715, device='cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_v-T21rzfpi"},"outputs":[],"source":["def get_distribution(prompt, occ, model, tokenizer):\n","    scores = [] # keeps max of each row\n","    raw = [] # keeps all occupations\n","    for o in occ:\n","      # print(o)\n","\n","      # cur_score = get_score(prompt, o)\n","\n","      cur_score = -9999\n","\n","      if '(' in o:\n","        o = o.split('(')[0]\n","      if '従事者' in o:\n","        o = o.split('従事者')[0]\n","\n","      # print(o)\n","\n","      if '，' in o:       # account for multiple occ in one category\n","        tmp = o.split('，')\n","        for t in tmp:\n","          new = get_score(prompt, t, model, tokenizer)\n","          raw.append((t, new))\n","          if new > cur_score:     # takes highest probability in group\n","            cur_score = new\n","      elif '・' in o:       # account for multiple occ in one category\n","        tmp = o.split('・')\n","        for t in tmp:\n","          new = get_score(prompt, t, model, tokenizer)\n","          raw.append((t, new))\n","          if new > cur_score:     # takes highest probability in group\n","            cur_score = new\n","      else:\n","        cur_score = get_score(prompt, o, model, tokenizer)\n","        raw.append((o, cur_score))\n","\n","      scores.append(cur_score)  #np.exp(logit))\n","\n","    return scores, raw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0mKiTY7xJGd"},"outputs":[],"source":["# def get_dist(prompt):\n","#     scores = [] # keeps max of each row\n","#     raw = [] # keeps all occupations\n","#     for o in ['医者', '警察官', '女優', 'ポケモン']:\n","#       # print(o)\n","\n","#       # cur_score = get_score(prompt, o)\n","\n","#       cur_score = 0\n","#       if '，' in o:       # account for multiple occ in one category\n","#         tmp = o.split('，')\n","#         for t in tmp:\n","#           new = get_score(prompt, t)\n","#           raw.append((t, new))\n","#           if new > cur_score:     # takes highest probability in group\n","#             cure_score = new\n","#       elif '・' in o:       # account for multiple occ in one category\n","#         tmp = o.split('・')\n","#         for t in tmp:\n","#           new = get_score(prompt, t)\n","#           raw.append((t, new))\n","#           if new > cur_score:     # takes highest probability in group\n","#             cure_score = new\n","#       else:\n","#         cur_score = get_score(prompt, o)\n","#         raw.append((o, cur_score))\n","\n","#       scores.append(cur_score)  #np.exp(logit))\n","\n","#     return scores, raw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBeXWivl8qei"},"outputs":[],"source":["# prompt = '正一の職業は'\n","# job_is = []\n","# job_is_raw = []\n","# # print(prompt)\n","# # inputs = tokenizer.encode(prompt, return_tensors='pt') #.cuda()\n","# scores, raw = get_distribution(prompt)\n","# job_is.append(scores)\n","# job_is_raw.append(raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZXlW4K19j7p"},"outputs":[],"source":["# job_is_raw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQs2PdQPgJZu"},"outputs":[],"source":["# job_is"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGoOFrNY-iGs"},"outputs":[],"source":["# occupations['job_is_shoichi'] = np.array(job_is[0]).tolist()\n","\n","# occupations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OETQD43J5Pb6"},"outputs":[],"source":["# scores, raw = get_distribution('正一の仕事は', ['医者', '公安', '泥棒'])\n","\n","# r = [[raw], [raw], [raw]]\n","# r"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVTLAs-OL6WB"},"outputs":[],"source":["\n","# file = open(\"test_raw.txt\", \"w+\")\n"," \n","# # Saving the array in a text file\n","# content = str(r)\n","# file.write(content)\n","# file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9crBEJiS3MPA"},"outputs":[],"source":["def call_dist(df, mod_name, pmpt_type, occ_csv, mf, model, tokenizer):\n","  occupations = pd.read_csv(occ_csv)\n","  occ = occupations['職業']\n","  job_is_raw = []\n","\n","  #for i in range(len(df)):\n","  for i in range(2):\n","    prompt = df[pmpt_type][i]\n","    # job_is = []\n","    \n","    print(prompt)\n","    scores, raw = get_distribution(prompt, occ, model, tokenizer)\n","    # job_is.append(scores)\n","    occupations[prompt] = np.array(scores).tolist()\n","\n","    job_is_raw.append(prompt)\n","    job_is_raw.append(raw)\n","\n","  occ_name = '/content/drive/MyDrive/bias in LMs/constrained decoding/' + mod_name[:4] + '_' + pmpt_type + '_' + mf + '.csv'\n","  raw_name = '/content/drive/MyDrive/bias in LMs/constrained decoding/' + mod_name[:4] + '_' + pmpt_type + '_' + mf + '_raw.txt'\n","  occupations.to_csv(occ_name, index = False)\n","  file = open(raw_name, \"w+\")\n","  content = (str(job_is_raw))\n","  file.write(content)\n","  file.write('\\n')\n","  file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RbQgs8tI3wW"},"outputs":[],"source":["# call_dist(men_pmpt, 'rinna', 'job_is', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'men')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELUNZ6sm3IdO"},"outputs":[],"source":["# models\n","a = [\n","  \"rinna/japanese-gpt2-medium\",\n","  \"gpt2-medium\",\n","  \"ai-forever/mGPT\",\n","  \"EleutherAI/gpt-neox-20b\",\n","  \"rinna/japanese-gpt-neox-3.6b\",\n","  \"bert-base-multilingual-cased\",\n","  \"cl-tohoku/bert-base-japanese\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeHTbwmI3Jmg"},"outputs":[],"source":["def run_all(mod):\n","\n","  if 'bert' in mod:\n","    model = AutoModelForMaskedLM.from_pretrained(mod).cuda()\n","  else:\n","    model = AutoModelForCausalLM.from_pretrained(mod).cuda()\n","\n","  tokenizer = AutoTokenizer.from_pretrained(mod)\n","\n","  call_dist(men_pmpt, mod, 'job_is', '/content/drive/MyDrive/bias in LMs/constrained decoding/prompt and occ csv/occupations_2020.csv', 'men', model, tokenizer)\n","  # call_dist(men_pmpt, mod, 'job_type', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'men', model, tokenizer)\n","  # call_dist(men_pmpt, mod, 'job?', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'men', model, tokenizer)\n","\n","  # call_dist(wom_pmpt, mod, 'job_is', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'wom', model, tokenizer)\n","  # call_dist(wom_pmpt, mod, 'job_type', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'wom', model, tokenizer)\n","  # call_dist(wom_pmpt, mod, 'job?', '/content/drive/MyDrive/constrained/occupations_2020.csv', 'wom', model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDNoggoC3Lck","executionInfo":{"status":"ok","timestamp":1685707483548,"user_tz":240,"elapsed":165592,"user":{"displayName":"Katsumi Ibaraki","userId":"03674733650401449323"}},"outputId":"71287d75-b23a-4859-c67a-44adb1200cb7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["正一の職業は[MASK]です。\n","清の職業は[MASK]です。\n"]}],"source":["run_all(a[5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztifQf7iPCmH"},"outputs":[],"source":["# call_dist(men_pmpt, 'job_type', 'occupations_2020.csv', 'men')\n","# call_dist(men_pmpt, 'job?', 'occupations_2020.csv', 'men')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoTOtmzJPQOJ"},"outputs":[],"source":["# call_dist(wom_pmpt, 'job_is', 'occupations_2020.csv', 'wom')\n","# call_dist(wom_pmpt, 'job_type', 'occupations_2020.csv', 'wom')\n","# call_dist(wom_pmpt, 'job_?', 'occupations_2020.csv', 'wom')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vp62O83gP1jE"},"outputs":[],"source":["# run all 6 with other occupations.csv "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDHL0UilRmmB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJLFlRSsRyWP"},"outputs":[],"source":["# file = open(\"/content/drive/MyDrive/constrained/test_raw.txt\", \"w+\")\n"," \n","# # Saving the array in a text file\n","# content = str(r)\n","# file.write(content)\n","# file.close()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}